"""Exposes the json data files in the /src/tokens directory. These files contain
lists of lists of tokens ie. [[tokens][tokens]]. The tokens lists [tokens]
are generated from files in the /src/corpora directory. The lists of lists
[[...][...]] are generated by combining tokens from different sources.

Metaphorically:

A text is a person.
Word tokens are that person's genes.
Many people form a gene pool.
A list of lists of tokens is a gene pool.
Each list in the list of lists (gene pool) is an individual.
"""
import json
import nltk
import os
import random

# Lists of lists of tokens: [[tokens][tokens]]
primary_tokens = []
secondary_tokens = []

corpora_dir = '../src/corpora/'
tokens_dir = '../src/tokens/'


def sequence_genes():
    """An alias."""
    tokenize_corpora()


def make_a_gene_pool(pool, individuals):
    """An alias."""
    make_tokens_dir(pool, individuals)


def add_genes_to_a_pool(pool, filters):
    """An alias."""
    save_tokens_dir_dir(pool, filters)


def choose_gene_pool(dir_):
    """An alias."""
    set_tokens_lists(dir_)


def tokenize_corpora():
    """ TODO """
    print("Tokenizing *.txt files in /src/corpora...")
    text_files = [fname for fname in os.listdir(corpora_dir) \
        if fname.split('.')[1] == 'txt']

    for text_fname in text_files:
        json_fname = text_fname.split('.')[0] + '.json'
        if os.path.isfile(corpora_dir + json_fname): continue

        print("Tokenizing " + text_fname)

        text = open(corpora_dir + text_fname).read()
        words = nltk.word_tokenize(text)
        with open(corpora_dir + json_fname, 'w') as outjson:
            json.dump(words, outjson)

    print("Done.")


def list_tokenized_corpora():
    """Retrieve the filenames of all tokenized text files in /src/corpora."""
    return [f for f in os.listdir(corpora_dir) if f.split('.')[1] == 'json']


def make_tokens_dir(dir_, sources):
    """Make a directory named <dir_> and write <sources> to a new file inside
    that directory. It will be called sources.json.
    """
    os.mkdir(tokens_dir + dir_)
    for source in sources:
        if not os.path.isfile(corpora_dir + source):
            print('Invalid source: ' + source)
            return

    with open(tokens_dir + dir_ + '/sources.json', 'w') as outjson:
        json.dump(sources, outjson)


#TODO: filter out stopwords
def save_tokens_to_dir(dir_, filters):
    """Read tokens from the files in sources.json. Filter each list of tokens
    and append it to a list of tokens lists. Write that list of lists to disk.
    """
    tokens_lists = []
    sources = []
    with open(tokens_dir + dir_ + '/sources.json', 'r') as injson:
        data = json.load(injson)
        sources = [corpora_dir + fname for fname in data]

    for fname in sources:
        print("Incorporating tokens from " + fname)
        with open(fname, 'r') as injson:
            data = json.load(injson)
            words = list(set([w.lower() for w in data if not w == '']))
            filtered = [w for w,p in nltk.pos_tag(words) if p in filters]
            tokens_lists.append(filtered)

    target = tokens_dir + dir_ + '/' + '-'.join(filters) + '.json'
    with open(target, 'w') as outjson:
        json.dump(tokens_lists, outjson)


def set_tokens_lists(dir_=None):
    """TODO"""
    if not dir_: dir_ = random.choice(os.listdir(tokens_dir))
    print("Loading tokens from: " + dir_)

    p_fname = tokens_dir + dir_ + '/NNS.json'
    s_fname = tokens_dir + dir_ + '/VBG.json'

    global primary_tokens
    primary_tokens = json.load(open(p_fname, 'r'))

    global secondary_tokens
    secondary_tokens = json.load(open(s_fname, 'r'))
